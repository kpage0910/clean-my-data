"""
Autonomous Data Cleaning Engine

STRICT MODE (Default):
This engine operates as a DETERMINISTIC data-cleaning system that:
1. NEVER invents, fabricates, or guesses new data
2. Only applies meaning-preserving transformations
3. Does NOT impute missing values unless explicitly enabled
4. Leaves invalid/blank cells blank or returns "Unknown" placeholder
5. Never changes the semantic meaning of a value
6. Only applies deterministic and reversible modifications

Allowed Transformations:
- Capitalization normalization (e.g., DANIEL → Daniel)
- Number formatting (e.g., "thirty" → 30)
- Date normalization (format standardization)
- Trimming whitespace
- Removing invalid/non-printable characters
- Lossless type conversions

Forbidden Transformations (in strict mode):
- Guessing names, emails, locations, ages, genders
- Replacing missing entries with fabricated content (e.g., "Unknown Person 1")
- Inferring values not directly derivable from input
- Marking invalid data with fabricated markers (e.g., "INVALID_EMAIL")
- Any transformation that changes semantic meaning

This module provides autonomous data cleaning capabilities that:
- Infer column types automatically
- Generate cleaning rules dynamically based on best practices
- Apply ONLY deterministic transformations
- Flag unsafe inferences
- Provide comprehensive validation reports
"""

import re
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Set
from enum import Enum

import numpy as np
import pandas as pd
from pydantic import BaseModel, Field

from app.models.schemas import CleaningRule, StrictModeConfig, DEFAULT_STRICT_CONFIG
from app.services.cleaner import apply_cleaning_rules, set_strict_config


# ============================================
# Type Inference System
# ============================================

class InferredType(str, Enum):
    """Enumeration of inferred column types."""
    NAME = "name"
    EMAIL = "email"
    PHONE = "phone"
    DATE = "date"
    NUMERIC = "numeric"
    CURRENCY = "currency"
    PERCENTAGE = "percentage"
    CATEGORICAL = "categorical"
    BOOLEAN = "boolean"
    TEXT = "text"
    ID = "id"
    ADDRESS = "address"
    URL = "url"
    UNKNOWN = "unknown"


class ColumnInference(BaseModel):
    """Result of column type inference."""
    column: str = Field(..., description="Column name")
    inferred_type: InferredType = Field(..., description="Detected column type")
    confidence: float = Field(..., description="Confidence score 0-1")
    indicators: List[str] = Field(default_factory=list, description="Evidence for inference")
    is_safe: bool = Field(True, description="Whether inference is safe to apply")
    warning: Optional[str] = Field(None, description="Warning message if inference is unsafe")


class GeneratedRule(BaseModel):
    """A cleaning rule generated by the autonomous engine."""
    rule: CleaningRule = Field(..., description="The cleaning rule to apply")
    reason: str = Field(..., description="Why this rule was generated")
    column: Optional[str] = Field(None, description="Target column")
    priority: int = Field(1, description="Execution priority (lower = earlier)")
    is_safe: bool = Field(True, description="Whether this rule is safe to apply automatically")


class ValidationIssue(BaseModel):
    """A single validation issue found during cleaning."""
    column: str = Field(..., description="Column with the issue")
    row_indices: List[int] = Field(default_factory=list, description="Affected row indices")
    issue_type: str = Field(..., description="Type of issue")
    description: str = Field(..., description="Human-readable description")
    original_values: List[Any] = Field(default_factory=list, description="Sample of original values")
    fixed: bool = Field(False, description="Whether the issue was fixed")
    fix_description: Optional[str] = Field(None, description="How it was fixed")


class AutonomousCleaningResult(BaseModel):
    """Complete result of autonomous cleaning operation."""
    summary: Dict[str, Any] = Field(..., description="Summary of detected issues")
    generated_rules: List[GeneratedRule] = Field(..., description="Rules that were generated")
    validation_report: Dict[str, Any] = Field(..., description="What was fixed and what couldn't be")
    column_inferences: List[ColumnInference] = Field(..., description="Column type inferences")
    warnings: List[str] = Field(default_factory=list, description="Warnings about unsafe operations")
    rows_processed: int = Field(..., description="Number of rows processed")
    columns_processed: int = Field(..., description="Number of columns processed")


# ============================================
# Pattern Definitions for Type Inference
# ============================================

# Name patterns
NAME_COLUMN_PATTERNS = re.compile(
    r'(^name$|^first.?name$|^last.?name$|^full.?name$|^customer.?name$|^user.?name$|'
    r'^person$|^contact$|^employee$|^author$|^owner$)',
    re.IGNORECASE
)

# Email patterns
EMAIL_PATTERN = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$')
EMAIL_COLUMN_PATTERNS = re.compile(r'(email|e.?mail|mail)', re.IGNORECASE)

# Phone patterns
PHONE_PATTERN = re.compile(r'^[\+]?[(]?[0-9]{1,3}[)]?[-\s\.]?[(]?[0-9]{1,4}[)]?[-\s\.]?[0-9]{1,4}[-\s\.]?[0-9]{1,9}$')
PHONE_COLUMN_PATTERNS = re.compile(r'(phone|telephone|tel|mobile|cell|fax)', re.IGNORECASE)

# Date patterns
DATE_COLUMN_PATTERNS = re.compile(
    r'(date|time|created|updated|modified|born|dob|birthday|timestamp|'
    r'start|end|expire|expiry|deadline|due|scheduled)',
    re.IGNORECASE
)

# Numeric/Currency/Percentage patterns
CURRENCY_PATTERN = re.compile(r'^[\$€£¥₹₽₩₪฿]\s*-?[\d,]+\.?\d*$')
PERCENTAGE_PATTERN = re.compile(r'^-?[\d,]+\.?\d*\s*%$')
NUMERIC_COLUMN_PATTERNS = re.compile(
    r'(amount|price|cost|total|sum|count|qty|quantity|number|num|'
    r'age|score|rating|weight|height|size|length|width|distance|'
    r'salary|income|revenue|budget|balance|payment|fee|rate|value)',
    re.IGNORECASE
)

# ID patterns
ID_COLUMN_PATTERNS = re.compile(r'(^id$|_id$|^.*_id$|^code$|^key$|^uuid$|^guid$)', re.IGNORECASE)

# Boolean patterns  
BOOLEAN_COLUMN_PATTERNS = re.compile(
    r'(^is_|^has_|^can_|^should_|^active$|^enabled$|^disabled$|^valid$|'
    r'^verified$|^confirmed$|^approved$|^deleted$)',
    re.IGNORECASE
)
BOOLEAN_VALUES = {'true', 'false', 'yes', 'no', '1', '0', 'y', 'n', 't', 'f'}

# Categorical patterns - columns that are likely categorical
CATEGORICAL_COLUMN_PATTERNS = re.compile(
    r'(status|type|category|state|level|priority|grade|rank|tier|group|class|kind|'
    r'gender|country|region|department|role|stage)',
    re.IGNORECASE
)

# URL patterns
URL_PATTERN = re.compile(r'^(https?://|www\.)[^\s]+$', re.IGNORECASE)
URL_COLUMN_PATTERNS = re.compile(r'(url|link|website|href|uri)', re.IGNORECASE)

# Address patterns
ADDRESS_COLUMN_PATTERNS = re.compile(
    r'(address|street|city|state|zip|postal|country|region|location)',
    re.IGNORECASE
)

# Number word mapping for conversion (single words only - compound parsing is in cleaner.py)
NUMBER_WORDS = {
    'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,
    'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10,
    'eleven': 11, 'twelve': 12, 'thirteen': 13, 'fourteen': 14, 'fifteen': 15,
    'sixteen': 16, 'seventeen': 17, 'eighteen': 18, 'nineteen': 19, 'twenty': 20,
    'thirty': 30, 'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70,
    'eighty': 80, 'ninety': 90, 'hundred': 100, 'thousand': 1000,
    'million': 1000000, 'billion': 1000000000,
}

# Import the compound number phrase parser from cleaner
from app.services.cleaner import is_number_phrase, parse_number_phrase


# ============================================
# Type Inference Engine
# ============================================

def infer_column_type(df: pd.DataFrame, column: str) -> ColumnInference:
    """
    Infer the semantic type of a column based on its name and values.
    
    Uses multiple signals:
    - Column name patterns
    - Value patterns and formats
    - Statistical properties
    
    Returns a ColumnInference with type, confidence, and safety assessment.
    """
    col_name = column
    col_data = df[column].dropna()
    indicators = []
    
    if len(col_data) == 0:
        return ColumnInference(
            column=column,
            inferred_type=InferredType.UNKNOWN,
            confidence=0.0,
            indicators=["Column is empty"],
            is_safe=False,
            warning="Column contains no data"
        )
    
    # Sample values for pattern matching
    sample_values = col_data.head(100).astype(str).tolist()
    
    # Check for ID column first (high priority)
    if ID_COLUMN_PATTERNS.search(col_name):
        indicators.append(f"Column name '{col_name}' matches ID pattern")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.ID,
            confidence=0.9,
            indicators=indicators,
            is_safe=True
        )
    
    # Check for Email
    if EMAIL_COLUMN_PATTERNS.search(col_name):
        indicators.append(f"Column name '{col_name}' suggests email")
    email_matches = sum(1 for v in sample_values if EMAIL_PATTERN.match(v.strip()))
    if email_matches > len(sample_values) * 0.5:
        indicators.append(f"{email_matches}/{len(sample_values)} values match email format")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.EMAIL,
            confidence=min(0.95, email_matches / len(sample_values)),
            indicators=indicators,
            is_safe=True
        )
    
    # Check for Phone
    if PHONE_COLUMN_PATTERNS.search(col_name):
        indicators.append(f"Column name '{col_name}' suggests phone number")
        phone_matches = sum(1 for v in sample_values if PHONE_PATTERN.match(v.strip().replace(' ', '')))
        if phone_matches > len(sample_values) * 0.3:
            return ColumnInference(
                column=column,
                inferred_type=InferredType.PHONE,
                confidence=0.8,
                indicators=indicators,
                is_safe=True
            )
    
    # Check for URL
    if URL_COLUMN_PATTERNS.search(col_name):
        indicators.append(f"Column name '{col_name}' suggests URL")
    url_matches = sum(1 for v in sample_values if URL_PATTERN.match(v.strip()))
    if url_matches > len(sample_values) * 0.5:
        indicators.append(f"{url_matches}/{len(sample_values)} values match URL format")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.URL,
            confidence=0.9,
            indicators=indicators,
            is_safe=True
        )
    
    # Check for Boolean
    if BOOLEAN_COLUMN_PATTERNS.search(col_name):
        indicators.append(f"Column name '{col_name}' suggests boolean")
    unique_values = set(str(v).lower().strip() for v in sample_values)
    if unique_values.issubset(BOOLEAN_VALUES) and len(unique_values) <= 3:
        indicators.append(f"Values {unique_values} are boolean-like")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.BOOLEAN,
            confidence=0.95,
            indicators=indicators,
            is_safe=True
        )
    
    # Check for Date
    if DATE_COLUMN_PATTERNS.search(col_name):
        indicators.append(f"Column name '{col_name}' suggests date/time")
        date_parse_success = 0
        for v in sample_values[:50]:
            try:
                pd.to_datetime(v)
                date_parse_success += 1
            except:
                pass
        if date_parse_success > len(sample_values[:50]) * 0.5:
            indicators.append(f"{date_parse_success}/{len(sample_values[:50])} values parse as dates")
            return ColumnInference(
                column=column,
                inferred_type=InferredType.DATE,
                confidence=0.85,
                indicators=indicators,
                is_safe=True
            )
    
    # Check for Currency
    currency_matches = sum(1 for v in sample_values if CURRENCY_PATTERN.match(v.strip()))
    if currency_matches > len(sample_values) * 0.3:
        indicators.append(f"{currency_matches}/{len(sample_values)} values have currency format")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.CURRENCY,
            confidence=0.9,
            indicators=indicators,
            is_safe=True
        )
    
    # Check for Percentage
    pct_matches = sum(1 for v in sample_values if PERCENTAGE_PATTERN.match(v.strip()))
    if pct_matches > len(sample_values) * 0.3:
        indicators.append(f"{pct_matches}/{len(sample_values)} values have percentage format")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.PERCENTAGE,
            confidence=0.9,
            indicators=indicators,
            is_safe=True
        )
    
    # Check for Name columns
    if NAME_COLUMN_PATTERNS.search(col_name):
        indicators.append(f"Column name '{col_name}' suggests person name")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.NAME,
            confidence=0.85,
            indicators=indicators,
            is_safe=True
        )
    
    # Check for Address
    if ADDRESS_COLUMN_PATTERNS.search(col_name):
        indicators.append(f"Column name '{col_name}' suggests address")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.ADDRESS,
            confidence=0.8,
            indicators=indicators,
            is_safe=True
        )
    
    # Check for Numeric (by column name)
    if NUMERIC_COLUMN_PATTERNS.search(col_name):
        indicators.append(f"Column name '{col_name}' suggests numeric data")
        
        # Count values that are either numeric OR number phrases
        numeric_count = 0
        number_phrase_count = 0
        
        for v in sample_values:
            v_str = str(v).strip()
            # Check if it's a number phrase first
            if is_number_phrase(v_str):
                number_phrase_count += 1
                numeric_count += 1
            else:
                # Try to parse as numeric (handles "50000", "75000", etc.)
                try:
                    # Remove common formatting
                    cleaned = v_str.replace(',', '').replace('$', '').replace('€', '').replace('£', '')
                    float(cleaned)
                    numeric_count += 1
                except (ValueError, TypeError):
                    pass
        
        # If more than 30% are numeric (including number phrases), treat as numeric
        if numeric_count > len(sample_values) * 0.3:
            if number_phrase_count > 0:
                indicators.append(f"{numeric_count}/{len(sample_values)} values are numeric ({number_phrase_count} are number phrases)")
            else:
                indicators.append(f"{numeric_count}/{len(sample_values)} values are numeric")
            return ColumnInference(
                column=column,
                inferred_type=InferredType.NUMERIC,
                confidence=0.85,
                indicators=indicators,
                is_safe=True
            )
    
    # Check if already numeric dtype
    if pd.api.types.is_numeric_dtype(df[column]):
        indicators.append("Column has numeric dtype")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.NUMERIC,
            confidence=0.95,
            indicators=indicators,
            is_safe=True
        )
    
    # Check for number phrases even without numeric column name pattern
    # This catches columns like "data" or "value" that contain number words/phrases
    number_phrase_count = sum(
        1 for v in sample_values 
        if is_number_phrase(str(v).strip())
    )
    if number_phrase_count > len(sample_values) * 0.5:
        indicators.append(f"{number_phrase_count}/{len(sample_values)} values are number phrases (e.g., 'sixty thousand')")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.NUMERIC,
            confidence=0.8,
            indicators=indicators,
            is_safe=True
        )
    
    # Check for categorical (low cardinality OR column name suggests categorical)
    unique_ratio = df[column].nunique() / len(df[column]) if len(df[column]) > 0 else 1
    is_low_cardinality = unique_ratio < 0.2 and df[column].nunique() < 50
    is_categorical_name = CATEGORICAL_COLUMN_PATTERNS.search(col_name) is not None
    
    if is_low_cardinality or is_categorical_name:
        if is_categorical_name:
            indicators.append(f"Column name '{col_name}' suggests categorical data")
        if is_low_cardinality:
            indicators.append(f"Low cardinality ({df[column].nunique()} unique values)")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.CATEGORICAL,
            confidence=0.8 if is_categorical_name else 0.7,
            indicators=indicators,
            is_safe=True
        )
    
    # Default to TEXT for string columns
    if df[column].dtype == object:
        indicators.append("String column with no specific pattern detected")
        return ColumnInference(
            column=column,
            inferred_type=InferredType.TEXT,
            confidence=0.5,
            indicators=indicators,
            is_safe=True
        )
    
    return ColumnInference(
        column=column,
        inferred_type=InferredType.UNKNOWN,
        confidence=0.3,
        indicators=["No specific type pattern detected"],
        is_safe=False,
        warning="Unable to determine column type with confidence"
    )


def infer_all_column_types(df: pd.DataFrame) -> List[ColumnInference]:
    """Infer types for all columns in the DataFrame."""
    return [infer_column_type(df, col) for col in df.columns]


# ============================================
# Autonomous Rule Generation
# ============================================

def generate_name_rules(column: str, df: pd.DataFrame) -> List[GeneratedRule]:
    """Generate cleaning rules for name columns."""
    rules = []
    col_data = df[column].dropna()
    
    # Rule 1: Trim whitespace
    rules.append(GeneratedRule(
        rule=CleaningRule(rule_type="trim_whitespace", column=column, params={}),
        reason="Remove leading/trailing whitespace from names",
        column=column,
        priority=1,
        is_safe=True
    ))
    
    # Rule 2: Normalize capitalization (e.g., DANIEL -> Daniel)
    rules.append(GeneratedRule(
        rule=CleaningRule(
            rule_type="normalize_capitalization", 
            column=column, 
            params={"style": "title"}
        ),
        reason="Normalize name capitalization to Title Case (meaning-preserving)",
        column=column,
        priority=2,
        is_safe=True
    ))
    
    return rules


def generate_email_rules(column: str, df: pd.DataFrame) -> List[GeneratedRule]:
    """Generate cleaning rules for email columns."""
    rules = []
    
    # Rule 1: Trim whitespace
    rules.append(GeneratedRule(
        rule=CleaningRule(rule_type="trim_whitespace", column=column, params={}),
        reason="Remove whitespace from emails",
        column=column,
        priority=1,
        is_safe=True
    ))
    
    # Rule 2: Normalize to lowercase (emails are case-insensitive)
    rules.append(GeneratedRule(
        rule=CleaningRule(
            rule_type="normalize_capitalization", 
            column=column, 
            params={"style": "lower"}
        ),
        reason="Normalize email to lowercase (emails are case-insensitive, meaning-preserving)",
        column=column,
        priority=2,
        is_safe=True
    ))
    
    return rules


def generate_date_rules(column: str, df: pd.DataFrame) -> List[GeneratedRule]:
    """Generate cleaning rules for date columns."""
    rules = []
    
    # Rule: Parse dates
    rules.append(GeneratedRule(
        rule=CleaningRule(rule_type="parse_dates", column=column, params={"errors": "coerce"}),
        reason="Standardize date format to ISO 8601",
        column=column,
        priority=2,
        is_safe=True
    ))
    
    return rules


def generate_numeric_rules(column: str, df: pd.DataFrame) -> List[GeneratedRule]:
    """Generate cleaning rules for numeric columns."""
    rules = []
    
    # Rule 1: Convert number words to digits (e.g., "thirty" -> 30)
    # This must run BEFORE normalize_numbers to avoid losing the values
    rules.append(GeneratedRule(
        rule=CleaningRule(
            rule_type="convert_number_words",
            column=column,
            params={}
        ),
        reason="Convert number words to digits (e.g., 'thirty' -> 30)",
        column=column,
        priority=1,
        is_safe=True
    ))
    
    # Rule 2: Normalize numbers (remove formatting)
    rules.append(GeneratedRule(
        rule=CleaningRule(
            rule_type="normalize_numbers", 
            column=column, 
            params={"remove_currency": True, "remove_commas": True}
        ),
        reason="Normalize numeric formatting (remove commas, currency symbols)",
        column=column,
        priority=2,
        is_safe=True
    ))
    
    # Rule 3: Coerce to numeric type
    rules.append(GeneratedRule(
        rule=CleaningRule(rule_type="coerce_types", column=column, params={"dtype": "float"}),
        reason="Convert to numeric type",
        column=column,
        priority=3,
        is_safe=True
    ))
    
    return rules


def generate_currency_rules(column: str, df: pd.DataFrame) -> List[GeneratedRule]:
    """Generate cleaning rules for currency columns."""
    rules = []
    
    # Rule 1: Normalize numbers (remove currency symbols and commas)
    rules.append(GeneratedRule(
        rule=CleaningRule(
            rule_type="normalize_numbers", 
            column=column, 
            params={"remove_currency": True, "remove_commas": True}
        ),
        reason="Remove currency symbols and format as numeric",
        column=column,
        priority=2,
        is_safe=True
    ))
    
    return rules


def generate_percentage_rules(column: str, df: pd.DataFrame) -> List[GeneratedRule]:
    """Generate cleaning rules for percentage columns."""
    rules = []
    
    # Rule 1: Normalize numbers (convert percentages)
    rules.append(GeneratedRule(
        rule=CleaningRule(
            rule_type="normalize_numbers", 
            column=column, 
            params={"remove_percent": True}
        ),
        reason="Convert percentage to decimal",
        column=column,
        priority=2,
        is_safe=True
    ))
    
    return rules


def generate_categorical_rules(column: str, df: pd.DataFrame) -> List[GeneratedRule]:
    """Generate cleaning rules for categorical columns."""
    rules = []
    
    # Rule 1: Trim whitespace
    rules.append(GeneratedRule(
        rule=CleaningRule(rule_type="trim_whitespace", column=column, params={}),
        reason="Normalize category values by trimming whitespace",
        column=column,
        priority=1,
        is_safe=True
    ))
    
    return rules


def generate_boolean_rules(column: str, df: pd.DataFrame) -> List[GeneratedRule]:
    """Generate cleaning rules for boolean columns."""
    rules = []
    
    # Rule: Coerce to bool type
    rules.append(GeneratedRule(
        rule=CleaningRule(rule_type="coerce_types", column=column, params={"dtype": "bool"}),
        reason="Standardize boolean values",
        column=column,
        priority=2,
        is_safe=True
    ))
    
    return rules


def generate_text_rules(column: str, df: pd.DataFrame) -> List[GeneratedRule]:
    """Generate cleaning rules for general text columns."""
    rules = []
    
    # Rule 1: Trim whitespace
    rules.append(GeneratedRule(
        rule=CleaningRule(rule_type="trim_whitespace", column=column, params={}),
        reason="Remove leading/trailing whitespace",
        column=column,
        priority=1,
        is_safe=True
    ))
    
    return rules


def generate_rules_for_column(
    inference: ColumnInference, 
    df: pd.DataFrame
) -> List[GeneratedRule]:
    """Generate all applicable cleaning rules for a column based on its inferred type."""
    
    rule_generators = {
        InferredType.NAME: generate_name_rules,
        InferredType.EMAIL: generate_email_rules,
        InferredType.DATE: generate_date_rules,
        InferredType.NUMERIC: generate_numeric_rules,
        InferredType.CURRENCY: generate_currency_rules,
        InferredType.PERCENTAGE: generate_percentage_rules,
        InferredType.CATEGORICAL: generate_categorical_rules,
        InferredType.BOOLEAN: generate_boolean_rules,
        InferredType.TEXT: generate_text_rules,
    }
    
    generator = rule_generators.get(inference.inferred_type)
    if generator:
        return generator(inference.column, df)
    
    return []


def generate_global_rules(df: pd.DataFrame) -> List[GeneratedRule]:
    """Generate global cleaning rules that apply to the entire dataset."""
    rules = []
    
    # Global normalize missing indicators (run first - converts na/N/A/- to null)
    rules.append(GeneratedRule(
        rule=CleaningRule(rule_type="normalize_missing_indicators", column=None, params={}),
        reason="Replace common missing-value indicators (na, N/A, -, etc.) with null",
        column=None,
        priority=0,  # Run first
        is_safe=True
    ))
    
    # Global trim whitespace for all string columns
    rules.append(GeneratedRule(
        rule=CleaningRule(rule_type="trim_whitespace", column=None, params={}),
        reason="Trim whitespace from all string columns",
        column=None,
        priority=1,  # Run second
        is_safe=True
    ))
    
    # Dedupe at the end
    rules.append(GeneratedRule(
        rule=CleaningRule(rule_type="dedupe", column=None, params={"keep": "first"}),
        reason="Remove duplicate rows",
        column=None,
        priority=100,  # Run last
        is_safe=True
    ))
    
    return rules


# ============================================
# Advanced Cleaning Transformations (Strict Mode Compliant)
# ============================================

def clean_name_column(
    series: pd.Series,
    strict_config: Optional[StrictModeConfig] = None,
) -> Tuple[pd.Series, List[ValidationIssue]]:
    """
    Clean a name column with STRICT MODE compliant transformations:
    - Convert to Title Case (meaning-preserving)
    - Strip whitespace (meaning-preserving)
    - Remove invalid characters (meaning-preserving)
    
    STRICT MODE: Does NOT fabricate placeholder names.
    Missing values remain as-is (blank/null).
    """
    if strict_config is None:
        strict_config = DEFAULT_STRICT_CONFIG
    
    issues = []
    result = series.copy()
    missing_count = 0
    
    def clean_name(val, idx):
        nonlocal missing_count
        
        if pd.isna(val) or str(val).strip() == '':
            missing_count += 1
            # STRICT MODE: Do NOT fabricate placeholder values
            # Leave blank cells blank
            return val  # Return original (None/NaN/empty)
        
        val_str = str(val).strip()
        
        # Remove invalid characters (keep letters, spaces, hyphens, apostrophes)
        # This is meaning-preserving - removes noise, not content
        cleaned = re.sub(r"[^a-zA-Z\s\-\']", '', val_str)
        
        # Convert to title case - meaning-preserving capitalization normalization
        cleaned = cleaned.title()
        
        # Collapse multiple spaces - meaning-preserving whitespace normalization
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # STRICT MODE: If cleaning results in empty string, preserve original
        # We don't fabricate data
        return cleaned if cleaned else val
    
    cleaned_values = []
    for idx, val in enumerate(result):
        cleaned_values.append(clean_name(val, idx))
    
    result = pd.Series(cleaned_values, index=result.index)
    
    # Get column name, use fallback if None
    col_name = series.name if series.name is not None else "unnamed_column"
    
    if missing_count > 0:
        issues.append(ValidationIssue(
            column=col_name,
            row_indices=[],
            issue_type="missing_names",
            description=f"Found {missing_count} missing/blank name values",
            fixed=False,  # STRICT MODE: Not fixed - we don't fabricate
            fix_description="Left blank - strict mode does not fabricate placeholder values"
        ))
    
    return result, issues


def clean_email_column(
    series: pd.Series,
    strict_config: Optional[StrictModeConfig] = None,
) -> Tuple[pd.Series, List[ValidationIssue]]:
    """
    Clean an email column with STRICT MODE compliant transformations:
    - Lowercase (meaning-preserving - email case is not significant)
    - Strip whitespace (meaning-preserving)
    - Validate format (detection only)
    
    STRICT MODE: Does NOT replace invalid emails with "INVALID_EMAIL" marker.
    Invalid emails are left as-is but flagged in the validation report.
    """
    if strict_config is None:
        strict_config = DEFAULT_STRICT_CONFIG
    
    issues = []
    result = series.copy()
    invalid_indices = []
    
    def clean_email(val, idx):
        if pd.isna(val) or str(val).strip() == '':
            return val
        
        val_str = str(val).strip().lower()
        
        # Validate email format - but don't replace invalid ones
        if not EMAIL_PATTERN.match(val_str):
            invalid_indices.append(idx)
            # STRICT MODE: Do NOT replace with fabricated marker
            # Just normalize case and whitespace, keep the original value structure
            return val_str  # Return normalized but not replaced
        
        return val_str
    
    cleaned_values = []
    for idx, val in enumerate(result):
        cleaned_values.append(clean_email(val, idx))
    
    result = pd.Series(cleaned_values, index=result.index)
    
    # Get column name, use fallback if None
    col_name = series.name if series.name is not None else "unnamed_column"
    
    if invalid_indices:
        issues.append(ValidationIssue(
            column=col_name,
            row_indices=invalid_indices[:10],
            issue_type="invalid_email",
            description=f"Found {len(invalid_indices)} invalid email formats",
            original_values=series.iloc[invalid_indices[:5]].tolist(),
            fixed=False,  # STRICT MODE: Not fixed - we don't fabricate markers
            fix_description="Left as-is - strict mode does not replace with fabricated markers"
        ))
    
    return result, issues


def clean_numeric_column(
    series: pd.Series,
    strict_config: Optional[StrictModeConfig] = None,
) -> Tuple[pd.Series, List[ValidationIssue]]:
    """
    Clean a numeric column with STRICT MODE compliant transformations:
    - Convert number words to digits (deterministic: "thirty" → 30)
    - Remove currency symbols and commas (meaning-preserving formatting)
    - Strip symbols (meaning-preserving)
    
    STRICT MODE: 
    - Does NOT guess or infer missing values
    - Does NOT apply risky typo corrections (O → 0, l → 1) as these can change meaning
    - Unparseable values are left as-is, not set to NaN
    """
    if strict_config is None:
        strict_config = DEFAULT_STRICT_CONFIG
    
    issues = []
    result = series.copy()
    conversion_issues = []
    
    def clean_numeric(val, idx):
        if pd.isna(val):
            return val
        
        val_str = str(val).strip().lower()
        
        # Check for number phrases (e.g., "sixty thousand", "two hundred fifty")
        # This is deterministic and meaning-preserving
        parsed_number = parse_number_phrase(val_str)
        if parsed_number is not None:
            return parsed_number
        
        # STRICT MODE: Do NOT apply risky typo corrections
        # "O" → "0" and "l" → "1" can change meaning (e.g., "Ol" is not "01")
        # We skip these transformations in strict mode
        
        # Remove currency symbols and commas - meaning-preserving formatting removal
        val_str = re.sub(r'[$€£¥,]', '', val_str)
        
        # Handle percentage signs - meaning-preserving conversion
        is_percent = '%' in val_str
        val_str = val_str.replace('%', '')
        
        try:
            result_val = float(val_str)
            if is_percent:
                result_val = result_val / 100
            return result_val
        except (ValueError, TypeError):
            conversion_issues.append(idx)
            # STRICT MODE: Do NOT set to NaN - leave original value
            return val  # Return original value, not NaN
    
    cleaned_values = []
    for idx, val in enumerate(result):
        cleaned_values.append(clean_numeric(val, idx))
    
    result = pd.Series(cleaned_values, index=result.index)
    
    # Get column name, use fallback if None
    col_name = series.name if series.name is not None else "unnamed_column"
    
    if conversion_issues:
        issues.append(ValidationIssue(
            column=col_name,
            row_indices=conversion_issues[:10],
            issue_type="numeric_conversion_failed",
            description=f"Could not convert {len(conversion_issues)} values to numeric",
            original_values=series.iloc[conversion_issues[:5]].tolist(),
            fixed=False,  # STRICT MODE: Not fixed - we don't force conversion
            fix_description="Left as-is - strict mode does not force lossy conversions"
        ))
    
    return result, issues


def clean_date_column(
    series: pd.Series,
    strict_config: Optional[StrictModeConfig] = None,
) -> Tuple[pd.Series, List[ValidationIssue]]:
    """
    Clean a date column with STRICT MODE compliant transformations:
    - Standardize to ISO format (YYYY-MM-DD) - meaning-preserving format change
    
    STRICT MODE: 
    - Unparseable dates are left as-is, not set to NaT
    - Does not fabricate or guess dates
    - Flag impossible dates
    """
    if strict_config is None:
        strict_config = DEFAULT_STRICT_CONFIG
    
    issues = []
    result = series.copy()
    invalid_indices = []
    
    # Try to parse dates with mixed format for better compatibility
    try:
        parsed = pd.to_datetime(result, errors='coerce', format='mixed')
    except Exception:
        # Fallback to default parsing if mixed isn't supported
        parsed = pd.to_datetime(result, errors='coerce')
    
    # Find which values failed to parse
    for idx in range(len(result)):
        if pd.notna(result.iloc[idx]) and pd.isna(parsed.iloc[idx]):
            invalid_indices.append(idx)
    
    # STRICT MODE: For unparseable dates, keep original values
    # Only convert successfully parsed dates to ISO format
    cleaned_values = []
    for idx in range(len(result)):
        if pd.isna(parsed.iloc[idx]):
            # Keep original value for unparseable dates
            cleaned_values.append(result.iloc[idx])
        else:
            # Convert to ISO format for valid dates
            cleaned_values.append(parsed.iloc[idx].strftime('%Y-%m-%d'))
    
    result = pd.Series(cleaned_values, index=result.index)
    
    # Get column name, use fallback if None
    col_name = series.name if series.name is not None else "unnamed_column"
    
    if invalid_indices:
        issues.append(ValidationIssue(
            column=col_name,
            row_indices=invalid_indices[:10],
            issue_type="invalid_date",
            description=f"Found {len(invalid_indices)} unparseable date values",
            original_values=series.iloc[invalid_indices[:5]].tolist(),
            fixed=False,  # STRICT MODE: Not fixed - we keep original values
            fix_description="Left as-is - strict mode does not discard unparseable values"
        ))
    
    return result, issues


def clean_categorical_column(
    series: pd.Series,
    strict_config: Optional[StrictModeConfig] = None,
) -> Tuple[pd.Series, List[ValidationIssue]]:
    """
    Clean a categorical column with STRICT MODE compliant transformations:
    - Normalize capitalization (meaning-preserving)
    - Merge case-different duplicates (meaning-preserving)
    
    STRICT MODE:
    - Does NOT fix misspellings (that would be guessing)
    - Does NOT replace values with inferred categories
    """
    if strict_config is None:
        strict_config = DEFAULT_STRICT_CONFIG
    
    issues = []
    result = series.copy()
    
    # Convert to string for processing, but preserve NaN
    def normalize_cat(val):
        if pd.isna(val):
            return val
        val_str = str(val).strip()
        if val_str.lower() in ('nan', 'none', '<na>', ''):
            return np.nan
        return val_str.title()  # Meaning-preserving capitalization normalization
    
    result = result.apply(normalize_cat)
    
    # Count merged categories
    original_unique = series.nunique()
    cleaned_unique = result.nunique()
    
    # Get column name, use fallback if None
    col_name = series.name if series.name is not None else "unnamed_column"
    
    if original_unique > cleaned_unique:
        issues.append(ValidationIssue(
            column=col_name,
            row_indices=[],
            issue_type="category_normalization",
            description=f"Merged {original_unique - cleaned_unique} duplicate categories (case normalization)",
            fixed=True,  # This is a meaning-preserving transformation
            fix_description="Normalized capitalization and merged case-different duplicates"
        ))
    
    return result, issues


# ============================================
# Main Autonomous Cleaning Engine (Strict Mode)
# ============================================

def autonomous_clean(
    df: pd.DataFrame,
    strict_config: Optional[StrictModeConfig] = None,
) -> Tuple[pd.DataFrame, AutonomousCleaningResult]:
    """
    Perform autonomous data cleaning on a DataFrame.
    
    STRICT MODE (Default):
    When strict_config.enabled is True (default):
    - Does NOT fabricate or guess new data
    - Only applies meaning-preserving transformations
    - Missing values remain missing (no imputation)
    - Invalid values are flagged but not replaced with markers
    
    This function:
    1. Infers column types automatically
    2. Generates cleaning rules based on best practices
    3. Applies ONLY deterministic transformations
    4. Flags unsafe inferences
    5. Returns cleaned dataset and comprehensive report
    
    Args:
        df: The input DataFrame to clean
        strict_config: Strict mode configuration (defaults to strict mode enabled)
        
    Returns:
        Tuple of (cleaned_df, AutonomousCleaningResult)
    """
    if strict_config is None:
        strict_config = DEFAULT_STRICT_CONFIG
    
    # Set the global strict config for all downstream operations
    set_strict_config(strict_config)
    
    # Track all issues and operations
    all_issues: List[ValidationIssue] = []
    all_warnings: List[str] = []
    generated_rules: List[GeneratedRule] = []
    
    # Add strict mode warning if enabled
    if strict_config.enabled:
        all_warnings.append(
            "STRICT MODE ENABLED: Only deterministic, meaning-preserving transformations applied. "
            "No data fabrication or imputation."
        )
    
    # Step 1: Infer column types
    column_inferences = infer_all_column_types(df)
    
    # Step 2: Generate global rules
    global_rules = generate_global_rules(df)
    generated_rules.extend(global_rules)
    
    # Step 3: Generate column-specific rules
    for inference in column_inferences:
        if not inference.is_safe:
            all_warnings.append(f"Column '{inference.column}': {inference.warning}")
        
        column_rules = generate_rules_for_column(inference, df)
        generated_rules.extend(column_rules)
    
    # Step 4: Sort rules by priority
    generated_rules.sort(key=lambda r: r.priority)
    
    # Step 5: Apply standard cleaning rules first (with strict config)
    standard_rules = [r.rule for r in generated_rules if r.is_safe]
    cleaned_df = apply_cleaning_rules(df.copy(), standard_rules, strict_config=strict_config)
    
    # Step 6: Apply advanced type-specific transformations (with strict config)
    for inference in column_inferences:
        col = inference.column
        if col not in cleaned_df.columns:
            continue
            
        if inference.inferred_type == InferredType.NAME:
            cleaned_df[col], issues = clean_name_column(cleaned_df[col], strict_config)
            all_issues.extend(issues)
            
        elif inference.inferred_type == InferredType.EMAIL:
            cleaned_df[col], issues = clean_email_column(cleaned_df[col], strict_config)
            all_issues.extend(issues)
            
        elif inference.inferred_type in (InferredType.NUMERIC, InferredType.CURRENCY, InferredType.PERCENTAGE):
            cleaned_df[col], issues = clean_numeric_column(cleaned_df[col], strict_config)
            all_issues.extend(issues)
            
        elif inference.inferred_type == InferredType.DATE:
            cleaned_df[col], issues = clean_date_column(cleaned_df[col], strict_config)
            all_issues.extend(issues)
            
        elif inference.inferred_type == InferredType.CATEGORICAL:
            cleaned_df[col], issues = clean_categorical_column(cleaned_df[col], strict_config)
            all_issues.extend(issues)
    
    # Step 7: Build summary
    issues_fixed = sum(1 for i in all_issues if i.fixed)
    issues_unfixed = sum(1 for i in all_issues if not i.fixed)
    
    summary = {
        "strict_mode": strict_config.enabled,
        "imputation_allowed": strict_config.allow_imputation,
        "total_rows": len(df),
        "total_columns": len(df.columns),
        "issues_detected": len(all_issues),
        "issues_fixed": issues_fixed,
        "issues_unfixed": issues_unfixed,
        "rules_applied": len(generated_rules),
        "column_types_inferred": {
            inf.column: inf.inferred_type.value for inf in column_inferences
        }
    }
    
    # Step 8: Build validation report
    validation_report = {
        "fixed": [
            {
                "column": i.column,
                "issue_type": i.issue_type,
                "description": i.description,
                "fix": i.fix_description
            }
            for i in all_issues if i.fixed
        ],
        "unfixed": [
            {
                "column": i.column,
                "issue_type": i.issue_type,
                "description": i.description,
                "reason": "Strict mode: cannot safely infer or fabricate correct value",
                "affected_rows": len(i.row_indices)
            }
            for i in all_issues if not i.fixed
        ],
        "warnings": all_warnings
    }
    
    result = AutonomousCleaningResult(
        summary=summary,
        generated_rules=generated_rules,
        validation_report=validation_report,
        column_inferences=column_inferences,
        warnings=all_warnings,
        rows_processed=len(df),
        columns_processed=len(df.columns)
    )
    
    return cleaned_df, result


def autonomous_scan_and_suggest(
    df: pd.DataFrame,
    strict_config: Optional[StrictModeConfig] = None,
) -> AutonomousCleaningResult:
    """
    Scan a DataFrame and generate cleaning suggestions without applying them.
    
    Useful for previewing what autonomous cleaning would do in strict mode.
    
    Args:
        df: The input DataFrame to analyze
        strict_config: Strict mode configuration (defaults to strict mode enabled)
        
    Returns:
        AutonomousCleaningResult with suggestions (not applied)
    """
    if strict_config is None:
        strict_config = DEFAULT_STRICT_CONFIG
    
    column_inferences = infer_all_column_types(df)
    generated_rules: List[GeneratedRule] = []
    all_warnings: List[str] = []
    
    # Add strict mode info
    if strict_config.enabled:
        all_warnings.append(
            "STRICT MODE: Only deterministic, meaning-preserving transformations will be suggested. "
            "No data fabrication or imputation."
        )
    
    # Generate global rules
    global_rules = generate_global_rules(df)
    generated_rules.extend(global_rules)
    
    # Generate column-specific rules
    for inference in column_inferences:
        if not inference.is_safe:
            all_warnings.append(f"Column '{inference.column}': {inference.warning}")
        
        column_rules = generate_rules_for_column(inference, df)
        generated_rules.extend(column_rules)
    
    generated_rules.sort(key=lambda r: r.priority)
    
    summary = {
        "strict_mode": strict_config.enabled,
        "imputation_allowed": strict_config.allow_imputation,
        "total_rows": len(df),
        "total_columns": len(df.columns),
        "rules_to_apply": len(generated_rules),
        "column_types_inferred": {
            inf.column: inf.inferred_type.value for inf in column_inferences
        }
    }
    
    return AutonomousCleaningResult(
        summary=summary,
        generated_rules=generated_rules,
        validation_report={"preview_mode": True, "strict_mode": strict_config.enabled},
        column_inferences=column_inferences,
        warnings=all_warnings,
        rows_processed=len(df),
        columns_processed=len(df.columns)
    )


def autonomous_preview(
    df: pd.DataFrame,
    n_rows: int = 100,
    strict_config: Optional[StrictModeConfig] = None,
) -> Tuple[List[dict], AutonomousCleaningResult]:
    """
    Generate a preview of autonomous cleaning changes.
    
    This runs the full autonomous cleaning on the first n_rows and returns
    the preview data in the format expected by the frontend PreviewTable.
    
    Args:
        df: The input DataFrame to preview
        n_rows: Number of rows to include in preview
        strict_config: Strict mode configuration (defaults to strict mode enabled)
        
    Returns:
        Tuple of (preview_rows, AutonomousCleaningResult)
        preview_rows is a list of dicts with row_index, original, cleaned, changes
    """
    if strict_config is None:
        strict_config = DEFAULT_STRICT_CONFIG
    
    # Get first n_rows for preview and reset index to ensure consistent indexing
    preview_df = df.head(n_rows).copy()
    preview_df = preview_df.reset_index(drop=True)
    
    # Run autonomous cleaning on preview data
    # Important: We need to preserve the index to match original vs cleaned rows
    cleaned_df, result = autonomous_clean(preview_df.copy(), strict_config=strict_config)
    
    # Create a mapping from cleaned index to cleaned row for proper matching
    # After dedupe, some rows may be removed, so we need to track which original rows remain
    cleaned_index_set = set(cleaned_df.index.tolist())
    
    # Generate preview rows comparing original vs cleaned
    preview_rows = []
    
    original_columns = set(preview_df.columns)
    cleaned_columns = set(cleaned_df.columns)
    
    def convert_to_native(val):
        """Convert numpy/pandas types to native Python types."""
        if pd.isna(val):
            return None
        if isinstance(val, (np.integer,)):
            return int(val)
        if isinstance(val, (np.floating,)):
            return float(val)
        if isinstance(val, np.bool_):
            return bool(val)
        if isinstance(val, np.ndarray):
            return val.tolist()
        
        return val
    
    def convert_row(row_dict):
        """Convert all values in a row dict to native Python types."""
        return {k: convert_to_native(v) for k, v in row_dict.items()}
    
    for idx in range(len(preview_df)):
        original_row = convert_row(preview_df.iloc[idx].to_dict())
        
        # Check if this row still exists in cleaned data (might be removed by dedupe)
        if idx in cleaned_index_set:
            cleaned_row = convert_row(cleaned_df.loc[idx].to_dict())
        else:
            # Row was removed (e.g., duplicate)
            cleaned_row = {}
        
        # Detect changes
        changes = []
        
        # Check for dropped columns
        for col in original_columns - cleaned_columns:
            changes.append(f"{col}: dropped")
        
        # If row was removed entirely
        if not cleaned_row:
            for col in original_columns:
                original_val = original_row.get(col)
                changes.append(f"{col}: '{original_val}' -> removed (duplicate)")
        else:
            # Check for value changes in remaining columns
            for col in cleaned_columns:
                original_val = original_row.get(col)
                cleaned_val = cleaned_row.get(col)
                
                # Compare values, handling None specially
                values_differ = False
                if original_val is None and cleaned_val is None:
                    values_differ = False
                elif original_val is None or cleaned_val is None:
                    values_differ = True
                elif original_val != cleaned_val:
                    values_differ = True
                
                if values_differ:
                    changes.append(f"{col}: '{original_val}' -> '{cleaned_val}'")
        
        preview_rows.append({
            "row_index": idx,
            "original": original_row,
            "cleaned": cleaned_row,
            "changes": changes,
        })
    
    # Update result with full dataset stats
    result.rows_processed = len(df)
    
    return preview_rows, result
